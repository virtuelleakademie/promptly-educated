[
  {
    "objectID": "slides/template.html#what-is-artifical-intelligence",
    "href": "slides/template.html#what-is-artifical-intelligence",
    "title": "Presentation Template",
    "section": "What is Artifical Intelligence?",
    "text": "What is Artifical Intelligence?\n\n\n\n\nA branch of computer science that aims to create machines that can perform tasks that typically require human intelligence."
  },
  {
    "objectID": "slides/template.html#what-is-a-large-language-model",
    "href": "slides/template.html#what-is-a-large-language-model",
    "title": "Presentation Template",
    "section": "What is a Large Language Model?",
    "text": "What is a Large Language Model?\n\n\n\n\nAn LLM is a type of generative AI model that is trained to predict the next word following the input (prompt)."
  },
  {
    "objectID": "slides/template.html#how-to-train-a-language-model",
    "href": "slides/template.html#how-to-train-a-language-model",
    "title": "Presentation Template",
    "section": "How to train a language model",
    "text": "How to train a language model\n\nAn LLM learns to predict the next word in a sequence, given the previous words: \\[ P(word | context) \\]\nThink of as ‚Äúfancy autocomplete‚Äù (but very very powerful and sopisticated)"
  },
  {
    "objectID": "slides/template.html#how-does-an-llm-generate-text",
    "href": "slides/template.html#how-does-an-llm-generate-text",
    "title": "Presentation Template",
    "section": "How does an LLM generate text?",
    "text": "How does an LLM generate text?"
  },
  {
    "objectID": "slides/template.html#sampling",
    "href": "slides/template.html#sampling",
    "title": "Presentation Template",
    "section": "Sampling",
    "text": "Sampling"
  },
  {
    "objectID": "slides/template.html#auto-regressive-generation",
    "href": "slides/template.html#auto-regressive-generation",
    "title": "Presentation Template",
    "section": "Auto-regressive generation",
    "text": "Auto-regressive generation\nText is generated one word at a time (actually tokens, not words).\n\n\n\n\n\n Generated text depends on the generative model and the context.\n Every word (token) is given an equal amount time (computation per token is constant)."
  },
  {
    "objectID": "slides/template.html#auto-regressive-generation-1",
    "href": "slides/template.html#auto-regressive-generation-1",
    "title": "Presentation Template",
    "section": "Auto-regressive generation",
    "text": "Auto-regressive generation"
  },
  {
    "objectID": "slides/template.html#foundation-models",
    "href": "slides/template.html#foundation-models",
    "title": "Presentation Template",
    "section": "Foundation models",
    "text": "Foundation models\nA foundation model, or large language model (LLM):\n\nis a type of machine learning model that is trained to predict the next word following the input (prompt).\nis trained ‚Äúsimply‚Äù to predict the next word following a sequence of words.\ndoes not necessarily produce human-like conversations.\n\n\n\n\n: What is the capital of France?\n: What is the capital of Germany? What is the capital of Italy? . .."
  },
  {
    "objectID": "slides/template.html#training-process",
    "href": "slides/template.html#training-process",
    "title": "Presentation Template",
    "section": "Training process",
    "text": "Training process\n\n\nFigure courtesy of Andrej Karpathy"
  },
  {
    "objectID": "slides/template.html#assistant-models",
    "href": "slides/template.html#assistant-models",
    "title": "Presentation Template",
    "section": "Assistant models",
    "text": "Assistant models\nTrained (fine-tuned) to have conversations: turn-taking, question answering, not being rude/sexist/racist.\n\n\n\n\n\n\nFoundation model has learned to predict all kinds of text, including both desirable and undesirable text.\nFine-tuning narrows down the space of all possible output to only desirable, human-like dialogue.\nModel is aligned with the values of the fine-tuner."
  },
  {
    "objectID": "slides/template.html#how-do-chatbots-work",
    "href": "slides/template.html#how-do-chatbots-work",
    "title": "Presentation Template",
    "section": "How do Chatbots work?",
    "text": "How do Chatbots work?\n\n\nDesigned to present the illusion of a conversation between two entities."
  },
  {
    "objectID": "slides/template.html#how-do-chatbots-actually-work",
    "href": "slides/template.html#how-do-chatbots-actually-work",
    "title": "Presentation Template",
    "section": "How do chatbots actually work?",
    "text": "How do chatbots actually work?"
  },
  {
    "objectID": "slides/template.html#an-assistant-model-is-a-conversation-simulator",
    "href": "slides/template.html#an-assistant-model-is-a-conversation-simulator",
    "title": "Presentation Template",
    "section": "An assistant model is a conversation simulator",
    "text": "An assistant model is a conversation simulator\n\n\n\n\n\nAn assistant is trained to respond to user prompts in a human-like way.\nSimulates possible human conversations.\nHas no intentions. It is not an entity with its own goals.\nDoes not have a ‚Äúpersonality‚Äù or ‚Äúcharacter‚Äù in the traditional sense. It can be thought of as a role-playing simulator.\nHas no concept of ‚Äútruth‚Äù or ‚Äúlying‚Äù. The model is not trying to deceive the user, it is simply trying to respond in a human-like way."
  },
  {
    "objectID": "slides/template.html#capabilities-and-limitations",
    "href": "slides/template.html#capabilities-and-limitations",
    "title": "Presentation Template",
    "section": "Capabilities and limitations",
    "text": "Capabilities and limitations\n\n\nWhat are LLMs good at?\n\nFixing grammar, bad writing, etc.\nRephrasing\nAnalyzing texts\nWriting computer code\nAnswering questions about a knowledge base\nTranslating languages\nCreating structured output\nFactual output with external documents or web search\n\n\nLimitations\n\nThey make stuff up (hallucinate)\nThey learn biases from the training data\nWeird vocabulary, e.g.¬†delve\n(Chatbots have privacy issues)"
  },
  {
    "objectID": "slides/template.html#hallucination",
    "href": "slides/template.html#hallucination",
    "title": "Presentation Template",
    "section": "Hallucination",
    "text": "Hallucination\n\n\n\n\n\nLLMs can generate text that is not true, or not based on any real-world knowledge.\nThis is known as ‚Äúhallucination‚Äù. A better term would be ‚Äúconfabulation‚Äù."
  },
  {
    "objectID": "slides/template.html#can-an-llm-tell-the-truth",
    "href": "slides/template.html#can-an-llm-tell-the-truth",
    "title": "Presentation Template",
    "section": "Can an LLM tell the truth?",
    "text": "Can an LLM tell the truth?\n\nHow would you know if an LLM is able to give you factual information?\nHow would you test this?\n\n\n\n\n: What is the capital of Uzbekistan?\n: Tashkent\n\n\n\nIt looks like the LLM knows the capital of Uzbekistan1.\nWhat it is actually doing is responding with the most likely sequence following the question."
  },
  {
    "objectID": "slides/template.html#knowledge-base",
    "href": "slides/template.html#knowledge-base",
    "title": "Presentation Template",
    "section": "Knowledge base",
    "text": "Knowledge base\n\n\n\nA knowledge base is a collection of facts about the world.\n\nYou can ask (retrieve) and tell (store) facts.\n\nAn LLM is not a knowledge base.\n\nLLMs generate text based on on how probable the next word is given the context, not based on stored facts."
  },
  {
    "objectID": "slides/template.html#biases",
    "href": "slides/template.html#biases",
    "title": "Presentation Template",
    "section": "Biases",
    "text": "Biases\n\n\n\n\n\n\n\n\nBiases in LLMs\nSource\nExamples\n\n\n\n\nTraining data bias\nText from internet, books, articles.\nStereotypes reflecting gender, race, religion.\n\n\nRepresentation bias\nUnderrepresented groups/perspectives in data.\nLess accurate responses for minority cultures.\n\n\nAlgorithmic bias\nTraining and fine-tuning algorithms.\nOptimizations for fluency and coherence may lead to preference for dominant cultural narratives.\n\n\nUser interaction bias\nAdaptation based on user interactions.\nIncreased biased or harmful content generation."
  },
  {
    "objectID": "slides/template.html#privacy-concerns",
    "href": "slides/template.html#privacy-concerns",
    "title": "Presentation Template",
    "section": "Privacy concerns",
    "text": "Privacy concerns\n\n\n\n\n\n\n\n\nPrivacy Concerns\nIssue\nExamples\n\n\n\n\nData memorization\nMemorizing sensitive information.\nReproducing phone numbers, addresses.\n\n\nTraining data leakage\nUnauthorized dissemination of confidential data.\nSummarizing proprietary documents.\n\n\nUser query logging\nStoring sensitive user interactions.\nExposing private queries if data is mishandled.\n\n\nQueries used for training\nUser queries may be used for further training.\nPersonal data in queries could be inadvertently included in training data."
  },
  {
    "objectID": "slides/template.html#prompting",
    "href": "slides/template.html#prompting",
    "title": "Presentation Template",
    "section": "Prompting",
    "text": "Prompting"
  },
  {
    "objectID": "slides/template.html#what-is-a-prompt",
    "href": "slides/template.html#what-is-a-prompt",
    "title": "Presentation Template",
    "section": "What is a prompt?",
    "text": "What is a prompt?\n\nAn LLM‚Äôs task is to complete text.\nA prompt is a piece of text (instruction) that is given to a language model to complete.\n\n\n\n\nPROMPT : Write a haiku about a workshop on large language models.\nASSISTANT : Whispers of circuits,\nKnowledge blooms in bytes and bits,\nModel learns and fits.\n\n\n\n\nThe response is generated as continuation of, and conditioned on, the prompt.\n\n\nMore technical definition: The output is generated by auto-regressively sampling from the probability distribution over the vocabulary, conditioned on the prompt."
  },
  {
    "objectID": "slides/template.html#prompt-engineering",
    "href": "slides/template.html#prompt-engineering",
    "title": "Presentation Template",
    "section": "Prompt engineering",
    "text": "Prompt engineering\n\n\n\n\n\nLLMs learn to do things they were not explicitly trained to do: translation, reasoning, etc. \nOften, these capabilities need to be ‚Äúunlocked‚Äù by the right prompt. \n\n\n\nBut what is the right prompt?\nThe answer is very similar to what you would tell a human dialogue partner/assistant.\nYou can increase the probability of getting the desired output by providing context and examples."
  },
  {
    "objectID": "slides/template.html#basics-of-prompting",
    "href": "slides/template.html#basics-of-prompting",
    "title": "Presentation Template",
    "section": "Basics of prompting",
    "text": "Basics of prompting\nOpenAI give a set of strategies for using their models effectively:\n Prompt engineering\nThese include:\n\nwriting clear instructions\nproviding reference texts\nsplitting tasks into subtasks\ngiving the LLM ‚Äòtime to think‚Äô\nusing external tools"
  },
  {
    "objectID": "slides/template.html#writing-clear-instructions",
    "href": "slides/template.html#writing-clear-instructions",
    "title": "Presentation Template",
    "section": "Writing clear instructions",
    "text": "Writing clear instructions\n\n\n\nInstructions should be clear and unambiguous.\nThink of an LLM as a role-playing conversation simulator: Indicate which role the model (persona) should adopt.\n\n\n\n\n\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output"
  },
  {
    "objectID": "slides/template.html#adopt-a-persona-role",
    "href": "slides/template.html#adopt-a-persona-role",
    "title": "Presentation Template",
    "section": "Adopt a persona (role)",
    "text": "Adopt a persona (role)\n\n\n\n: You are an expert on learning techniques. Explain the concept of ‚Äòflipped classroom‚Äô in one paragraph.\n\n\n\n\n\n\n: You are an expert financial derivatives. Explain the concept of ‚Äòflipped classroom‚Äô in one paragraph."
  },
  {
    "objectID": "slides/template.html#provide-reference-texts",
    "href": "slides/template.html#provide-reference-texts",
    "title": "Presentation Template",
    "section": "Provide reference texts",
    "text": "Provide reference texts\n\nProvide a model with trusted and relevant information.\nThen instruct the model to use the provided information to compose its answer.\n\n Instruct the model to answer using a reference text\n\n\n\nThis can be extended to retrieval-augmented generation (RAG). First create a database of documents, then retrieve the most relevant documents, based on a user‚Äôs query. These are then included in the prompt to the model. The model is instructed to use the information in the documents to compose its answer."
  },
  {
    "objectID": "slides/template.html#create-structured-output",
    "href": "slides/template.html#create-structured-output",
    "title": "Presentation Template",
    "section": "Create structured output",
    "text": "Create structured output\n\nExplanation: Instruct the model to generate structured output.\nE.g. provide a table, a list, a diagram, etc.\nUse delimiters to indicate distinct parts of the input.\nExample: Extract information from a text and present it in a table."
  },
  {
    "objectID": "slides/template.html#structured-prompting-techniques",
    "href": "slides/template.html#structured-prompting-techniques",
    "title": "Presentation Template",
    "section": "Structured prompting techniques",
    "text": "Structured prompting techniques\n\nIn-Context Learning: Provide examples within the prompt.\nThought Generation: Instruct the model to think step-by-step.\nDecomposition Techniques: Break down tasks into subtasks.\n\n(Schulhoff et al. 2024)"
  },
  {
    "objectID": "slides/template.html#in-context-learning",
    "href": "slides/template.html#in-context-learning",
    "title": "Presentation Template",
    "section": "In-Context learning",
    "text": "In-Context learning\n\nExplanation: Providing examples or context within the prompt itself.\nFew-shot prompting: Give a few examples.\n\nExample: Translate the following sentences:\n\nEnglish: ‚ÄòWhat time is it?‚Äô -&gt; French: ‚ÄòQuelle heure est-il?‚Äô\nEnglish: ‚ÄòWhere is the library?‚Äô -&gt; French:\n\n\nZero-shot prompting: No examples, relies on pre-trained knowledge.\n\nExample: Translate the following sentence‚Ä¶"
  },
  {
    "objectID": "slides/template.html#thought-generation",
    "href": "slides/template.html#thought-generation",
    "title": "Presentation Template",
    "section": "Thought generation",
    "text": "Thought generation\n\nExplanation: Encourages the model to show its reasoning process.\nChain-of-Thought (CoT) prompting: encourages the LLM to ‚Äúexplain‚Äù its intermediate reasoning steps.\nCan often be induced by simply instructing the model to think step-by-step or Take a deep breath and work on this problem step-by-step (Yang et al. 2023)."
  },
  {
    "objectID": "slides/template.html#chain-of-thought-example",
    "href": "slides/template.html#chain-of-thought-example",
    "title": "Presentation Template",
    "section": "Chain-of-Thought example",
    "text": "Chain-of-Thought example\nInstead of this:\n\n\n\n: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. Yes or no?\n\n\n\nDo this:\n\n\n\n: Is this statement correct? The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nReason through the problem step-by-step. Start by identifying the odd numbers. Next, add them up. Finally, determine if the sum is even or odd. Write down your reasoning steps in a numbered list.\n\n\n\n\nWhy does this work?"
  },
  {
    "objectID": "slides/template.html#decomposition-techniques",
    "href": "slides/template.html#decomposition-techniques",
    "title": "Presentation Template",
    "section": "Decomposition techniques",
    "text": "Decomposition techniques\n\nExplanation: Force the LLM to break down complex tasks into manageable subtasks.\nLeast-to-Most Prompting: Start simple, increase complexity.\n\nExample: List items, calculate cost‚Ä¶\n\nPlan-and-Solve Prompting: Separate planning and execution phases.\n\nExample: Understand the problem, devise a plan‚Ä¶"
  },
  {
    "objectID": "slides/template.html#hands-on-practice-prompting",
    "href": "slides/template.html#hands-on-practice-prompting",
    "title": "Presentation Template",
    "section": "Hands-on practice: Prompting",
    "text": "Hands-on practice: Prompting\n Open this activity.\n\nPractice writing prompts for different tasks ( 20 minutes).\nWrite an essay using an LLM, and then critique someone else‚Äôs essay ( 30 minutes).\n\n  If you need further help with prompting techniques, see these websites:\n\n Learn prompting\n Prompting guide\n OpenAI cookbook"
  },
  {
    "objectID": "slides/template.html#chatgpt-edu",
    "href": "slides/template.html#chatgpt-edu",
    "title": "Presentation Template",
    "section": "ChatGPT Edu",
    "text": "ChatGPT Edu\n\n\n \n\n\n\n\nAccess to GPT-4o, excelling in text interpretation, coding, and mathematics\nData analytics, web browsing, and document summarization\nBuild GPTs, custom versions of ChatGPT, and share them within university workspaces\nSignificantly higher message limits than the free version of ChatGPT\nImproved language capabilities across quality and speed, with over 50 languages supported\nRobust security, data privacy, and administrative controls\nConversations and data are not used to train OpenAI models"
  },
  {
    "objectID": "slides/template.html#gpts",
    "href": "slides/template.html#gpts",
    "title": "Presentation Template",
    "section": "GPTs",
    "text": "GPTs"
  },
  {
    "objectID": "slides/template.html#hands-on-practice-gpts",
    "href": "slides/template.html#hands-on-practice-gpts",
    "title": "Presentation Template",
    "section": "Hands-on practice: GPTs",
    "text": "Hands-on practice: GPTs\n\nTry out custom GPTs from various categories in the GPT store.\nDiscuss with your neighbour\n\nDid you discover any useful GPTs?\nWhat are the benefits and limitations of using GPTs in the classroom?"
  },
  {
    "objectID": "slides/template.html#extended-cognition",
    "href": "slides/template.html#extended-cognition",
    "title": "Presentation Template",
    "section": "Extended cognition",
    "text": "Extended cognition\n\n\n\nAccording to Clark and Chalmers (1998), cognitive processes may extend to external objects.\nKrakauer (2016) distinguishes between complementary and competitive cognitive artifacts.\n\nComplementary: numbers, abacus\nCompetetive: calculator, GPS\n\nWhat kind of artefact will AI turn out to be?"
  },
  {
    "objectID": "slides/template.html#deskilling-vs.-upskilling",
    "href": "slides/template.html#deskilling-vs.-upskilling",
    "title": "Presentation Template",
    "section": "Deskilling vs.¬†upskilling",
    "text": "Deskilling vs.¬†upskilling"
  },
  {
    "objectID": "slides/template.html#writing-tasks-in-the-ai-era",
    "href": "slides/template.html#writing-tasks-in-the-ai-era",
    "title": "Presentation Template",
    "section": "Writing tasks in the AI era",
    "text": "Writing tasks in the AI era\n\nWriting is a core skill: critical thinking, persuasion, argumentation, understanding.\nText creation is secondary in learning: focus is on underlying skills.\nLearning objectives: Benefits of writing tasks should be clearly and convincingly conveyed.\nStudents should be equipped for effective (controlled) use of AI."
  },
  {
    "objectID": "slides/template.html#ai-can-do-my-homework",
    "href": "slides/template.html#ai-can-do-my-homework",
    "title": "Presentation Template",
    "section": "AI can do my homework",
    "text": "AI can do my homework\n\nWe can think of this as cheating.\nMore useful: cheating means bypassing useful cognition and therefore missing out on learning.\nCheating an ethics problem.\nBypassing cognition is a learning problem.\nNot a new problem: books, encyclopedias, calculators, spell checkers, etc."
  },
  {
    "objectID": "slides/template.html#controlled-use-of-llms",
    "href": "slides/template.html#controlled-use-of-llms",
    "title": "Presentation Template",
    "section": "Controlled use of LLMs",
    "text": "Controlled use of LLMs\n\n\n\n\n\n\n\nTask Category\nSpecific Tasks\n\n\n\n\nEditing tasks\nCreate/improve different versions of sections.\n\n\nTransitions\nWrite and compare transitions.\n\n\nImprove drafts\nCritique and refine drafts.\n\n\nWriting styles\nRewrite sections for different audiences.\n\n\nControversial statements\nIdentify controversial points and strengthen arguments.\n\n\nResearch journal\nKeep a diary and use LLM for reflection."
  },
  {
    "objectID": "slides/template.html#sport-vs.-writing",
    "href": "slides/template.html#sport-vs.-writing",
    "title": "Presentation Template",
    "section": "Sport vs.¬†writing",
    "text": "Sport vs.¬†writing\n\n\n\nTechnological advancements in sports: a useful analogy for learning?\nDistinction between training and competition.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLZR Racer swim suit\nAI-base writing tools\n\n\n\n\nImprovement\nReduced Resistance, Increased Buoyancy\nImproved Grammar, Formulation, Content Creation\n\n\nFairness\nProvided an Unfair Advantage, Led to Record Performances\nConsidered Unfair in Academic Contexts\n\n\nImpact\nBanned to Maintain Competitive Integrity\nRaises Questions of Originality and Skill Development"
  },
  {
    "objectID": "slides/template.html#understanding-the-value-of-effort",
    "href": "slides/template.html#understanding-the-value-of-effort",
    "title": "Presentation Template",
    "section": "Understanding the value of effort",
    "text": "Understanding the value of effort\n\nCheating can be a symptom that learners do not understand or value the importance of their own work.\nJust like in sport: if we take shortcuts during training, we won‚Äôt get fit.\nUnderstanding the purpose is important to endure discomfort.\nLearners need to understand what they are supposed to learn, why it is valuable, and why effort and discomfort are necessary."
  },
  {
    "objectID": "slides/template.html#fraud-triangle",
    "href": "slides/template.html#fraud-triangle",
    "title": "Presentation Template",
    "section": "Fraud triangle",
    "text": "Fraud triangle"
  },
  {
    "objectID": "slides/template.html#learning-environments-that-promote-cheating",
    "href": "slides/template.html#learning-environments-that-promote-cheating",
    "title": "Presentation Template",
    "section": "Learning Environments that promote cheating",
    "text": "Learning Environments that promote cheating\n\n\n\n\n\n\n\nFactors\nDescriptions\n\n\n\n\nHigh pressure\nHigh stakes increase cheating. Fear of failure reinforces this.\n\n\nLack of intrinsic motivation\nEngagement and relevance are important. Lacking these makes cheating more attractive.\n\n\nPerceived injustice\nUnfair grading leads to cheating.\n\n\nLow fear of getting caught\nLow risk encourages cheating.\n\n\nPeer influence\nWidespread cheating among peers pressures students to join in.\n\n\nLow self-efficacy\nDoubts about one‚Äôs own abilities increase cheating as the seemingly only option."
  },
  {
    "objectID": "slides/template.html#strategies-to-reduce-cheating",
    "href": "slides/template.html#strategies-to-reduce-cheating",
    "title": "Presentation Template",
    "section": "Strategies to Reduce Cheating",
    "text": "Strategies to Reduce Cheating\n\n\n\n\n\n\n\nStrategies\nDescriptions\n\n\n\n\nFoster intrinsic motivation\nSpark genuine interest. Provide choices and practical applications.\n\n\nMastery learning\nClear learning objectives. Focus on mastery of content. Include constructive and corrective feedback in formative assessments.\n\n\nReduce pressure\nDiversify assessment methods. Use portfolios and low-stress tests to reduce anxiety.\n\n\nStrengthen self-efficacy\nProvide constructive feedback and promote peer learning (peer tutoring, peer review).\n\n\nCreate a culture of integrity\nOpen discussion about academic integrity. Set clear guidelines and promote community ethics."
  },
  {
    "objectID": "slides/template.html#academic-integrity-plagiarism",
    "href": "slides/template.html#academic-integrity-plagiarism",
    "title": "Presentation Template",
    "section": "Academic Integrity: Plagiarism",
    "text": "Academic Integrity: Plagiarism\n\n\n\n\n\n\n\nTypes of Plagiarism\nDescription\n\n\n\n\nUnattributed use\nUsing the work or ideas of others without proper attribution.\n\n\nMinor changes or translations\nUsing the work of others with minor changes or translations without attribution.\n\n\nSelf-plagiarism\nReusing substantial parts of one‚Äôs own work without proper citation.\n\n\nJoint works\nReusing jointly written publications without proper acknowledgment."
  },
  {
    "objectID": "slides/template.html#academic-integrity-misconduct-in-authorship",
    "href": "slides/template.html#academic-integrity-misconduct-in-authorship",
    "title": "Presentation Template",
    "section": "Academic Integrity: Misconduct in authorship",
    "text": "Academic Integrity: Misconduct in authorship\n\n\n\n\n\n\n\nTypes of Plagiarism\nDescription\n\n\n\n\nUnattributed use\nUsing the work or ideas of others without proper attribution.\n\n\nMinor changes or translations\nUsing the work of others with minor changes or translations without attribution.\n\n\nSelf-plagiarism\nReusing substantial parts of one‚Äôs own work without proper citation.\n\n\nJoint works\nReusing jointly written publications without proper acknowledgment."
  },
  {
    "objectID": "slides/template.html#how-to-cite-chatgpt",
    "href": "slides/template.html#how-to-cite-chatgpt",
    "title": "Presentation Template",
    "section": "How to cite ChatGPT",
    "text": "How to cite ChatGPT\nE.g. APA Style: Cite as software (not as personal communication)."
  },
  {
    "objectID": "slides/template.html#documentating-ai-use",
    "href": "slides/template.html#documentating-ai-use",
    "title": "Presentation Template",
    "section": "Documentating AI use",
    "text": "Documentating AI use\n\nSpecifying prompts works well for inexperienced users, but inadequately reflects complex processes.\nExperienced users work with dialogues and several tools, not monolithic prompts in ChatGPT.\nWorking with copilot (code): no traceable prompt input.\nInstead: Document the process, including the tools used and the steps taken.\n\nInclude used tools and steps in appendix, with optional graphical representation.\nServes both evaluation and self-reflection.\n\nIs documentation meaningful in the long term, once the use of AI-based tools has become commonplace?"
  },
  {
    "objectID": "slides/template.html#detecting-ai-use",
    "href": "slides/template.html#detecting-ai-use",
    "title": "Presentation Template",
    "section": "Detecting AI use",
    "text": "Detecting AI use\n\nCan be detected by the use of specific vocabulary and phrases: ‚Äúdelve‚Äù, ‚Äúvibrant‚Äù, ‚Äúembark‚Äù, ‚Äúit‚Äôs important to note‚Äù, ‚Äù based on the data provided‚Äù.\nDetection tools are not very useful, and can be easily circumvented.\nAccording to Fleckenstein et al. (2024)\n\nGenerative AI can write papers that are undetectable.\nTeachers overestimate their detection abilities."
  },
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Using AI in Education: Intermediate",
    "section": "",
    "text": "The workshop is divided into several blocks, each focusing on different aspects of LLM usage and prompt engineering. Here‚Äôs a brief overview of the structure:\n\n\n\n\n\n\nTime Check\n\n\n\n\n\nOpening Block (15 min)\nFoundations Block (45 min)\nBreak (15 min)\nIntermediate Block (45 min)\nImplementation Block (45 min)\nClosing (15 min)"
  },
  {
    "objectID": "pages/index.html#general-tips",
    "href": "pages/index.html#general-tips",
    "title": "Using AI in Education: Intermediate",
    "section": "General tips",
    "text": "General tips\nOpenAI give a set of  strategies for using their models. If you need examples, this might be a good place to start.\nThe strategies include:\n\nwriting clear instructions\nproviding reference texts\nsplitting tasks into subtasks\ngiving GPT ‚Äòtime to think‚Äô\nusing external tools\n\nSome general techniques are:\n\nNumbered Steps:: For sequential tasks, use numbered steps. This helps the model understand the sequence of actions.\nUse delimiters: To separate various parts of the prompt (e.g.¬†\", `,, ', |, #, ‚Ä¶).\nFew-shot prompting: Provide a few examples for guidance.\n\nCombining these techniques, a template prompt might look like this:\n\n\n\n\n\n\nTemplate\n\n\n\n\nRole: who is being simulated?\nTask: what is to be done?\nSteps: what are the steps to complete the task?\nContext: what is the context of the task?\nGoal: what is the goal of the task?\nFormat: what is the format of the output? How long should it be?\n\n\n\nRemember to structure your prompt in a way that is clear and easy to understand. You can use markdown to format your prompt, and you instruct the mode to format its response using markdown.\n\nExample\n\n\n\n\n\n\n Prompt:\n\n\n\nYou are an expert on the topic of university education and didactics. Explain the concept of ‚Äúflipped classroom‚Äù to a group of teachers. Use markdown to format your response. Keep you explanation concise and to the point.\n\n\nDifferent persona:\n\n\n\n\n\n\n Prompt:\n\n\n\nYou are 13 year old high school student. Explain the concept of ‚Äúflipped classroom‚Äù to your friends. Use markdown to format your response. Keep you explanation concise and to the point.\n\n\nAsk the model to output a table:\n\n\n\n\n\n\n Prompt:\n\n\n\nYou are a high school teacher. Give me a table containing Greek letters in one column, their pronunciation in the second column, and examples of usage in the third column. Give me a maximum of 6 rows."
  },
  {
    "objectID": "pages/index.html#structured-prompting-techniques",
    "href": "pages/index.html#structured-prompting-techniques",
    "title": "Using AI in Education: Intermediate",
    "section": "Structured prompting techniques",
    "text": "Structured prompting techniques\n\nIn-Context Learning: Provide examples within the prompt\n\nExplanation\nIn-Context Learning involves providing the language model with examples or context within the prompt itself. This technique helps guide the model‚Äôs responses by demonstrating the desired output format or type of information.\n\n\nTechniques\n\nFew-Shot Prompting: Provide a few examples of the desired output before asking for a new response.\nZero-Shot Prompting: Ask the model to perform a task without any examples, relying on its pre-trained knowledge.\n\n\n\nExample\n\n\n\n\n\n\n Few-Shot Prompting:\n\n\n\nSummarize the main findings of these research paper abstracts:\n\nAbstract: [Insert first abstract] Summary: The study found that increased physical activity is associated with improved cognitive function in older adults.\nAbstract: [Insert second abstract] Summary: The research demonstrated a positive correlation between employee satisfaction and productivity in remote work environments.\n\nNow summarize this new abstract: [Insert new abstract to be summarized]\n\n\n\n\n\n\n\n\nHands-on Exercise\n\n\n\n\nChoose a topic you‚Äôre interested in. If you would like to use an AI-based tool for literature research, you can use Elicit.\nCreate zero-shot and few-shot prompts to extract the research question, methodology, and main findings from the abstracts of three research papers.\n\n\n\n\n\n\nThought Generation: Instruct the model to think step-by-step\n\nExplanation\nThought generation techniques encourage the model to show its reasoning process, making the output more transparent and often more accurate.\n\n\nTechniques\nChain-of-Thought (CoT) Prompting: Ask the model to break down its thinking into steps.\n\nZero-Shot CoT: Request step-by-step reasoning without providing examples.\nFew-Shot CoT: Provide examples of step-by-step reasoning before asking for a new response.\n\n\n\nExample Prompt (Chain-of-Thought):\n\n\n\n\n\n\n Prompt:\n\n\n\nAnalyze the methodology of the following research study. Think through this step-by-step: 1. Identify the research design. 2. Evaluate the appropriateness of the chosen methods. 3. Assess potential limitations or biases. 4. Consider alternative approaches that could have been used.\n[Insert methodology section of a research paper]\n\n\n\n\n\nDecomposition Techniques: Break down tasks into subtasks\n\nExplanation\nDecomposition techniques involve breaking down complex tasks into smaller, more manageable subtasks. This approach can lead to more accurate and comprehensive responses.\n\n\nTechniques\n\nLeast-to-Most Prompting: Start with the simplest subtask and gradually increase complexity. More info here:  Least-to-Most Prompting\nPlan-and-Solve Prompting: Separate the task into a planning phase and an execution phase.\n\n\n\nExample Prompt (Plan-and-Solve):\n\n\n\n\n\n\n Prompt:\n\n\n\nWe need to conduct a systematic literature review on [specific topic]. Let‚Äôs approach this in two phases:\nPlanning Phase: 1. Outline the main steps needed for this systematic review. 2. For each step, briefly describe what needs to be considered.\nSolving Phase: Now, let‚Äôs address each step in detail, providing specific strategies and methodologies."
  },
  {
    "objectID": "slides/prompt-engineering-interediate.html#references",
    "href": "slides/prompt-engineering-interediate.html#references",
    "title": "Presentation 2",
    "section": "",
    "text": "Berner Fachhochschule | Bern University of Applied Sciences\n\n\n\n\nBowen, Jos√© Antonio, and C. Edward Watson. 2024. Teaching with AI. Johns Hopkins University Press. https://doi.org/10.56021/9781421449227.\n\n\nLang, James M. 2013. ‚ÄúCheating Lessons.‚Äù Harvard University Press. 2013. https://www.hup.harvard.edu/books/9780674724631.\n\n\nSchulhoff, Sander, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, et al. 2024. ‚ÄúThe Prompt Report: A Systematic Survey of Prompting Techniques.‚Äù June 6, 2024. http://arxiv.org/abs/2406.06608."
  },
  {
    "objectID": "pages/tools.html",
    "href": "pages/tools.html",
    "title": "Promptly Educated //",
    "section": "",
    "text": "ü§ó HuggingChat"
  },
  {
    "objectID": "pages/tools.html#open-assistants",
    "href": "pages/tools.html#open-assistants",
    "title": "Promptly Educated //",
    "section": "",
    "text": "ü§ó HuggingChat"
  },
  {
    "objectID": "pages/tools.html#ai-tools",
    "href": "pages/tools.html#ai-tools",
    "title": "Promptly Educated //",
    "section": "AI Tools",
    "text": "AI Tools\nüëâüèº The largest AI tools directory (updated daily)"
  },
  {
    "objectID": "pages/tools.html#literatursuche",
    "href": "pages/tools.html#literatursuche",
    "title": "Promptly Educated //",
    "section": "Literatursuche",
    "text": "Literatursuche\nüëâüèº Elicit\nüëâüèº Consensus"
  },
  {
    "objectID": "pages/tools.html#prompt-engineering",
    "href": "pages/tools.html#prompt-engineering",
    "title": "Promptly Educated //",
    "section": "Prompt Engineering",
    "text": "Prompt Engineering\nüëâüèº Prompting Guide"
  },
  {
    "objectID": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "href": "pages/resources.html#ki-orientierungshilfe-der-bfh",
    "title": "Virtuelle Akademie",
    "section": "KI Orientierungshilfe der BFH",
    "text": "KI Orientierungshilfe der BFH\n\nKI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nKI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base"
  },
  {
    "objectID": "pages/resources.html#bildung-6.0",
    "href": "pages/resources.html#bildung-6.0",
    "title": "Virtuelle Akademie",
    "section": "Bildung 6.0",
    "text": "Bildung 6.0\nDas Projekt Bildung 6.0 der Berner Fachhochschule stellt relevante und verl√§ssliche Informationen und Empfehlungen zum richtigen Umgang mit KI-basierten Werkzeugen (KBW) f√ºr Studierende und Lehrende auf einer Online-Plattform bereit.\n\nProjekt Bildung 6.0"
  },
  {
    "objectID": "pages/resources.html#chatgpt-an-der-hochschule",
    "href": "pages/resources.html#chatgpt-an-der-hochschule",
    "title": "Virtuelle Akademie",
    "section": "ChatGPT an der Hochschule",
    "text": "ChatGPT an der Hochschule\n\nPrompts for Education: Enhancing Productivity & Learning\nUnlocking the Power of Generative AI Models and Systems such as GPT-4 and ChatGPT for Higher Education\n√úberblick √ºber KI-Tools im Kontext von akademischen Lese- und Schreibprozessen\nChatGPT im Hochschulkontext ‚Äì eine kommentierte Linksammlung\nUni Bern: Chatbots in der Hochschullehre"
  },
  {
    "objectID": "pages/resources.html#youtube",
    "href": "pages/resources.html#youtube",
    "title": "Virtuelle Akademie",
    "section": "Youtube",
    "text": "Youtube\nLarge language models from scratch\n\nHow do large language models work: part 1\nHow do large language models work: part 2\n\nTutorials zu Prompting\n\nChatGPT Mega Prompts"
  },
  {
    "objectID": "pages/resources.html#how-does-gpt-work",
    "href": "pages/resources.html#how-does-gpt-work",
    "title": "Virtuelle Akademie",
    "section": "How does GPT work?",
    "text": "How does GPT work?\n\nGenerative AI exists because of the transformer (Financial Times 12/09/2023)"
  },
  {
    "objectID": "pages/resources.html#wissenschaftliches-arbeiten",
    "href": "pages/resources.html#wissenschaftliches-arbeiten",
    "title": "Virtuelle Akademie",
    "section": "Wissenschaftliches Arbeiten",
    "text": "Wissenschaftliches Arbeiten\n\nChatGPT zitieren\nRechtliche Fragen\nDidaktische Und Rechtliche Perspektiven Auf Ki-Gest√ºtztes Schreiben In Der Hochschulbildung (Salden 2023)"
  },
  {
    "objectID": "pages/resources.html#ethische-fragen",
    "href": "pages/resources.html#ethische-fragen",
    "title": "Virtuelle Akademie",
    "section": "Ethische Fragen",
    "text": "Ethische Fragen\n\nPrek√§re Klickarbeit hinter den Kulissen von ChatGPT\nTraumatische Klickarbeit: Die Menschen hinter ChatGPT"
  },
  {
    "objectID": "slides/prompt-engineering-basics.html#references",
    "href": "slides/prompt-engineering-basics.html#references",
    "title": "Presentation 2",
    "section": "",
    "text": "Berner Fachhochschule | Bern University of Applied Sciences\n\n\n\n\nBowen, Jos√© Antonio, and C. Edward Watson. 2024. Teaching with AI. Johns Hopkins University Press. https://doi.org/10.56021/9781421449227.\n\n\nLang, James M. 2013. ‚ÄúCheating Lessons.‚Äù Harvard University Press. 2013. https://www.hup.harvard.edu/books/9780674724631.\n\n\nSchulhoff, Sander, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, et al. 2024. ‚ÄúThe Prompt Report: A Systematic Survey of Prompting Techniques.‚Äù June 6, 2024. http://arxiv.org/abs/2406.06608."
  },
  {
    "objectID": "slides/understanding-llms.html#references",
    "href": "slides/understanding-llms.html#references",
    "title": "Understanding Large Language Models",
    "section": "References",
    "text": "References\n\n\nBowen, Jos√© Antonio, and C. Edward Watson. 2024. Teaching with AI. Johns Hopkins University Press. https://doi.org/10.56021/9781421449227.\n\n\nKambhampati, Subbarao. 2024. ‚ÄúCan Large Language Models Reason and Plan?‚Äù Annals of the New York Academy of Sciences 1534 (1): 15‚Äì18. https://doi.org/10.1111/nyas.15125.\n\n\nLang, James M. 2013. ‚ÄúCheating Lessons.‚Äù Harvard University Press. 2013. https://www.hup.harvard.edu/books/9780674724631.\n\n\nSchulhoff, Sander, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, et al. 2024. ‚ÄúThe Prompt Report: A Systematic Survey of Prompting Techniques.‚Äù June 6, 2024. http://arxiv.org/abs/2406.06608.\n\n\nShanahan, Murray, Kyle McDonell, and Laria Reynolds. 2023. ‚ÄúRole Play with Large Language Models.‚Äù Nature, November, 1‚Äì6. https://doi.org/10.1038/s41586-023-06647-8.\n\n\n\n\n\n\nBerner Fachhochschule | Bern University of Applied Sciences"
  },
  {
    "objectID": "slides/presentation-1.html#wie-machen-llms-das-12",
    "href": "slides/presentation-1.html#wie-machen-llms-das-12",
    "title": "Presentation 1",
    "section": "Wie machen LLMs das? 1/2",
    "text": "Wie machen LLMs das? 1/2\nLLM bewerten die Wahrscheinlichkeit eines Textes. Dabei lernen sie die Beziehungen zwischen W√∂rtern und S√§tzen.\n\n\n\n\n\n\n\nSatz\nWahrscheinlichkeit\n\n\n\n\nDie Katze sass auf der Bank. Die Bank war aus Holz\n90 %\n\n\nDie Katze sass auf der Bank. Die Bank war geschlossen\n10 %\n\n\n\n Training von LLMs:\n\nmit ALLEN qualitativ hochwertigen, weltweit zug√§nglichen menschlichen Sprachdaten\nUmfasst B√ºcher, Zeitungen, geschriebene Sprachen, Internet-Dialoge und transkribierte Video-Dialoge\n\nOpenAI brauchte \\(13\\cdot 10^{12}\\) Tokens (~100 Mio B√ºcher) um GPT-4 mit ca. \\(1\\cdot 10^{12}\\) Parameter, dh. Terabytes an Speicher, auf Tausenden von GPUs mehrere Monate zu trainieren.\nn-gram"
  },
  {
    "objectID": "slides/presentation-1.html#what-is-artifical-intelligence",
    "href": "slides/presentation-1.html#what-is-artifical-intelligence",
    "title": "Presentation 1",
    "section": "What is Artifical Intelligence?",
    "text": "What is Artifical Intelligence?\n\n\n\n\nA branch of computer science that aims to create machines that can perform tasks that typically require human intelligence."
  },
  {
    "objectID": "slides/presentation-1.html#what-is-a-large-language-model",
    "href": "slides/presentation-1.html#what-is-a-large-language-model",
    "title": "Presentation 1",
    "section": "What is a Large Language Model?",
    "text": "What is a Large Language Model?\n\n\n\n\nAn LLM is a type of generative AI model that is trained to predict the next word following the input (prompt)."
  },
  {
    "objectID": "slides/presentation-1.html#how-to-train-a-language-model",
    "href": "slides/presentation-1.html#how-to-train-a-language-model",
    "title": "Presentation 1",
    "section": "How to train a language model",
    "text": "How to train a language model\n\nAn LLM learns to predict the next word in a sequence, given the previous words: \\[ P(word | context) \\]\nThink of as ‚Äúfancy autocomplete‚Äù (but very very powerful and sopisticated)"
  },
  {
    "objectID": "slides/presentation-1.html#how-does-an-llm-generate-text",
    "href": "slides/presentation-1.html#how-does-an-llm-generate-text",
    "title": "Presentation 1",
    "section": "How does an LLM generate text?",
    "text": "How does an LLM generate text?"
  },
  {
    "objectID": "slides/presentation-1.html#sampling",
    "href": "slides/presentation-1.html#sampling",
    "title": "Presentation 1",
    "section": "Sampling",
    "text": "Sampling"
  },
  {
    "objectID": "slides/presentation-1.html#auto-regressive-generation",
    "href": "slides/presentation-1.html#auto-regressive-generation",
    "title": "Presentation 1",
    "section": "Auto-regressive generation",
    "text": "Auto-regressive generation\nText is generated one word at a time (actually tokens, not words).\n\n\n\n\n\n Generated text depends on the generative model and the context.\n Every word (token) is given an equal amount time (computation per token is constant)."
  },
  {
    "objectID": "slides/presentation-1.html#auto-regressive-generation-1",
    "href": "slides/presentation-1.html#auto-regressive-generation-1",
    "title": "Presentation 1",
    "section": "Auto-regressive generation",
    "text": "Auto-regressive generation"
  },
  {
    "objectID": "slides/presentation-1.html#foundation-models",
    "href": "slides/presentation-1.html#foundation-models",
    "title": "Presentation 1",
    "section": "Foundation models",
    "text": "Foundation models\nA foundation model, or large language model (LLM):\n\nis a type of machine learning model that is trained to predict the next word following the input (prompt).\nis trained ‚Äúsimply‚Äù to predict the next word following a sequence of words.\ndoes not necessarily produce human-like conversations.\n\n\n\n\n: What is the capital of France?\n: What is the capital of Germany? What is the capital of Italy? . .."
  },
  {
    "objectID": "slides/presentation-1.html#training-process",
    "href": "slides/presentation-1.html#training-process",
    "title": "Presentation 1",
    "section": "Training process",
    "text": "Training process\n\n\nFigure courtesy of Andrej Karpathy"
  },
  {
    "objectID": "slides/presentation-1.html#assistant-models",
    "href": "slides/presentation-1.html#assistant-models",
    "title": "Presentation 1",
    "section": "Assistant models",
    "text": "Assistant models\nTrained (fine-tuned) to have conversations: turn-taking, question answering, not being rude/sexist/racist.\n\n\n\n\n\n\nFoundation model has learned to predict all kinds of text, including both desirable and undesirable text.\nFine-tuning narrows down the space of all possible output to only desirable, human-like dialogue.\nModel is aligned with the values of the fine-tuner."
  },
  {
    "objectID": "slides/presentation-1.html#how-do-chatbots-work",
    "href": "slides/presentation-1.html#how-do-chatbots-work",
    "title": "Presentation 1",
    "section": "How do Chatbots work?",
    "text": "How do Chatbots work?\n\n\nDesigned to present the illusion of a conversation between two entities."
  },
  {
    "objectID": "slides/presentation-1.html#how-do-chatbots-actually-work",
    "href": "slides/presentation-1.html#how-do-chatbots-actually-work",
    "title": "Presentation 1",
    "section": "How do chatbots actually work?",
    "text": "How do chatbots actually work?"
  },
  {
    "objectID": "slides/presentation-1.html#an-assistant-model-is-a-conversation-simulator",
    "href": "slides/presentation-1.html#an-assistant-model-is-a-conversation-simulator",
    "title": "Presentation 1",
    "section": "An assistant model is a conversation simulator",
    "text": "An assistant model is a conversation simulator\n\n\n\n\n\nAn assistant is trained to respond to user prompts in a human-like way.\nSimulates possible human conversations.\nHas no intentions. It is not an entity with its own goals.\nDoes not have a ‚Äúpersonality‚Äù or ‚Äúcharacter‚Äù in the traditional sense. It can be thought of as a role-playing simulator.\nHas no concept of ‚Äútruth‚Äù or ‚Äúlying‚Äù. The model is not trying to deceive the user, it is simply trying to respond in a human-like way."
  },
  {
    "objectID": "slides/presentation-1.html#capabilities-and-limitations",
    "href": "slides/presentation-1.html#capabilities-and-limitations",
    "title": "Presentation 1",
    "section": "Capabilities and limitations",
    "text": "Capabilities and limitations\n\n\nWhat are LLMs good at?\n\nFixing grammar, bad writing, etc.\nRephrasing\nAnalyzing texts\nWriting computer code\nAnswering questions about a knowledge base\nTranslating languages\nCreating structured output\nFactual output with external documents or web search\n\n\nLimitations\n\nThey make stuff up (hallucinate)\nThey learn biases from the training data\nWeird vocabulary, e.g.¬†delve\n(Chatbots have privacy issues)"
  },
  {
    "objectID": "slides/presentation-1.html#hallucination",
    "href": "slides/presentation-1.html#hallucination",
    "title": "Presentation 1",
    "section": "Hallucination",
    "text": "Hallucination\n\n\n\n\n\nLLMs can generate text that is not true, or not based on any real-world knowledge.\nThis is known as ‚Äúhallucination‚Äù. A better term would be ‚Äúconfabulation‚Äù."
  },
  {
    "objectID": "slides/presentation-1.html#can-an-llm-tell-the-truth",
    "href": "slides/presentation-1.html#can-an-llm-tell-the-truth",
    "title": "Presentation 1",
    "section": "Can an LLM tell the truth?",
    "text": "Can an LLM tell the truth?\n\nHow would you know if an LLM is able to give you factual information?\nHow would you test this?\n\n\n\n\n: What is the capital of Uzbekistan?\n: Tashkent\n\n\n\nIt looks like the LLM knows the capital of Uzbekistan1.\nWhat it is actually doing is responding with the most likely sequence following the question."
  },
  {
    "objectID": "slides/presentation-1.html#knowledge-base",
    "href": "slides/presentation-1.html#knowledge-base",
    "title": "Presentation 1",
    "section": "Knowledge base",
    "text": "Knowledge base\n\n\n\nA knowledge base is a collection of facts about the world.\n\nYou can ask (retrieve) and tell (store) facts.\n\nAn LLM is not a knowledge base.\n\nLLMs generate text based on on how probable the next word is given the context, not based on stored facts."
  },
  {
    "objectID": "slides/presentation-1.html#biases",
    "href": "slides/presentation-1.html#biases",
    "title": "Presentation 1",
    "section": "Biases",
    "text": "Biases\n\n\n\n\n\n\n\n\nBiases in LLMs\nSource\nExamples\n\n\n\n\nTraining data bias\nText from internet, books, articles.\nStereotypes reflecting gender, race, religion.\n\n\nRepresentation bias\nUnderrepresented groups/perspectives in data.\nLess accurate responses for minority cultures.\n\n\nAlgorithmic bias\nTraining and fine-tuning algorithms.\nOptimizations for fluency and coherence may lead to preference for dominant cultural narratives.\n\n\nUser interaction bias\nAdaptation based on user interactions.\nIncreased biased or harmful content generation."
  },
  {
    "objectID": "slides/presentation-1.html#privacy-concerns",
    "href": "slides/presentation-1.html#privacy-concerns",
    "title": "Presentation 1",
    "section": "Privacy concerns",
    "text": "Privacy concerns\n\n\n\n\n\n\n\n\nPrivacy Concerns\nIssue\nExamples\n\n\n\n\nData memorization\nMemorizing sensitive information.\nReproducing phone numbers, addresses.\n\n\nTraining data leakage\nUnauthorized dissemination of confidential data.\nSummarizing proprietary documents.\n\n\nUser query logging\nStoring sensitive user interactions.\nExposing private queries if data is mishandled.\n\n\nQueries used for training\nUser queries may be used for further training.\nPersonal data in queries could be inadvertently included in training data."
  },
  {
    "objectID": "slides/presentation-1.html#prompting",
    "href": "slides/presentation-1.html#prompting",
    "title": "Presentation 1",
    "section": "Prompting",
    "text": "Prompting"
  },
  {
    "objectID": "slides/presentation-1.html#what-is-a-prompt",
    "href": "slides/presentation-1.html#what-is-a-prompt",
    "title": "Presentation 1",
    "section": "What is a prompt?",
    "text": "What is a prompt?\n\nAn LLM‚Äôs task is to complete text.\nA prompt is a piece of text (instruction) that is given to a language model to complete.\n\n\n\n\nPROMPT : Write a haiku about a workshop on large language models.\nASSISTANT : Whispers of circuits,\nKnowledge blooms in bytes and bits,\nModel learns and fits.\n\n\n\n\nThe response is generated as continuation of, and conditioned on, the prompt.\n\n\nMore technical definition: The output is generated by auto-regressively sampling from the probability distribution over the vocabulary, conditioned on the prompt."
  },
  {
    "objectID": "slides/presentation-1.html#prompt-engineering",
    "href": "slides/presentation-1.html#prompt-engineering",
    "title": "Presentation 1",
    "section": "Prompt engineering",
    "text": "Prompt engineering\n\n\n\n\n\nLLMs learn to do things they were not explicitly trained to do: translation, reasoning, etc. \nOften, these capabilities need to be ‚Äúunlocked‚Äù by the right prompt. \n\n\n\nBut what is the right prompt?\nThe answer is very similar to what you would tell a human dialogue partner/assistant.\nYou can increase the probability of getting the desired output by providing context and examples."
  },
  {
    "objectID": "slides/presentation-1.html#basics-of-prompting",
    "href": "slides/presentation-1.html#basics-of-prompting",
    "title": "Presentation 1",
    "section": "Basics of prompting",
    "text": "Basics of prompting\nOpenAI give a set of strategies for using their models effectively:\n Prompt engineering\nThese include:\n\nwriting clear instructions\nproviding reference texts\nsplitting tasks into subtasks\ngiving the LLM ‚Äòtime to think‚Äô\nusing external tools"
  },
  {
    "objectID": "slides/presentation-1.html#writing-clear-instructions",
    "href": "slides/presentation-1.html#writing-clear-instructions",
    "title": "Presentation 1",
    "section": "Writing clear instructions",
    "text": "Writing clear instructions\n\n\n\nInstructions should be clear and unambiguous.\nThink of an LLM as a role-playing conversation simulator: Indicate which role the model (persona) should adopt.\n\n\n\n\n\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output"
  },
  {
    "objectID": "slides/presentation-1.html#adopt-a-persona-role",
    "href": "slides/presentation-1.html#adopt-a-persona-role",
    "title": "Presentation 1",
    "section": "Adopt a persona (role)",
    "text": "Adopt a persona (role)\n\n\n\n: You are an expert on learning techniques. Explain the concept of ‚Äòflipped classroom‚Äô in one paragraph.\n\n\n\n\n\n\n: You are an expert financial derivatives. Explain the concept of ‚Äòflipped classroom‚Äô in one paragraph."
  },
  {
    "objectID": "slides/presentation-1.html#provide-reference-texts",
    "href": "slides/presentation-1.html#provide-reference-texts",
    "title": "Presentation 1",
    "section": "Provide reference texts",
    "text": "Provide reference texts\n\nProvide a model with trusted and relevant information.\nThen instruct the model to use the provided information to compose its answer.\n\n Instruct the model to answer using a reference text\n\n\n\nThis can be extended to retrieval-augmented generation (RAG). First create a database of documents, then retrieve the most relevant documents, based on a user‚Äôs query. These are then included in the prompt to the model. The model is instructed to use the information in the documents to compose its answer."
  },
  {
    "objectID": "slides/presentation-1.html#create-structured-output",
    "href": "slides/presentation-1.html#create-structured-output",
    "title": "Presentation 1",
    "section": "Create structured output",
    "text": "Create structured output\n\nExplanation: Instruct the model to generate structured output.\nE.g. provide a table, a list, a diagram, etc.\nUse delimiters to indicate distinct parts of the input.\nExample: Extract information from a text and present it in a table."
  },
  {
    "objectID": "slides/presentation-1.html#structured-prompting-techniques",
    "href": "slides/presentation-1.html#structured-prompting-techniques",
    "title": "Presentation 1",
    "section": "Structured prompting techniques",
    "text": "Structured prompting techniques\n\nIn-Context Learning: Provide examples within the prompt.\nThought Generation: Instruct the model to think step-by-step.\nDecomposition Techniques: Break down tasks into subtasks.\n\n(Schulhoff et al. 2024)"
  },
  {
    "objectID": "slides/presentation-1.html#in-context-learning",
    "href": "slides/presentation-1.html#in-context-learning",
    "title": "Presentation 1",
    "section": "In-Context learning",
    "text": "In-Context learning\n\nExplanation: Providing examples or context within the prompt itself.\nFew-shot prompting: Give a few examples.\n\nExample: Translate the following sentences:\n\nEnglish: ‚ÄòWhat time is it?‚Äô -&gt; French: ‚ÄòQuelle heure est-il?‚Äô\nEnglish: ‚ÄòWhere is the library?‚Äô -&gt; French:\n\n\nZero-shot prompting: No examples, relies on pre-trained knowledge.\n\nExample: Translate the following sentence‚Ä¶"
  },
  {
    "objectID": "slides/presentation-1.html#thought-generation",
    "href": "slides/presentation-1.html#thought-generation",
    "title": "Presentation 1",
    "section": "Thought generation",
    "text": "Thought generation\n\nExplanation: Encourages the model to show its reasoning process.\nChain-of-Thought (CoT) prompting: encourages the LLM to ‚Äúexplain‚Äù its intermediate reasoning steps.\nCan often be induced by simply instructing the model to think step-by-step or Take a deep breath and work on this problem step-by-step (Yang et al. 2023)."
  },
  {
    "objectID": "slides/presentation-1.html#chain-of-thought-example",
    "href": "slides/presentation-1.html#chain-of-thought-example",
    "title": "Presentation 1",
    "section": "Chain-of-Thought example",
    "text": "Chain-of-Thought example\nInstead of this:\n\n\n\n: The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1. Yes or no?\n\n\n\nDo this:\n\n\n\n: Is this statement correct? The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\nReason through the problem step-by-step. Start by identifying the odd numbers. Next, add them up. Finally, determine if the sum is even or odd. Write down your reasoning steps in a numbered list.\n\n\n\n\nWhy does this work?"
  },
  {
    "objectID": "slides/presentation-1.html#decomposition-techniques",
    "href": "slides/presentation-1.html#decomposition-techniques",
    "title": "Presentation 1",
    "section": "Decomposition techniques",
    "text": "Decomposition techniques\n\nExplanation: Force the LLM to break down complex tasks into manageable subtasks.\nLeast-to-Most Prompting: Start simple, increase complexity.\n\nExample: List items, calculate cost‚Ä¶\n\nPlan-and-Solve Prompting: Separate planning and execution phases.\n\nExample: Understand the problem, devise a plan‚Ä¶"
  },
  {
    "objectID": "slides/presentation-1.html#hands-on-practice-prompting",
    "href": "slides/presentation-1.html#hands-on-practice-prompting",
    "title": "Presentation 1",
    "section": "Hands-on practice: Prompting",
    "text": "Hands-on practice: Prompting\n Open this activity.\n\nPractice writing prompts for different tasks ( 20 minutes).\nWrite an essay using an LLM, and then critique someone else‚Äôs essay ( 30 minutes).\n\n  If you need further help with prompting techniques, see these websites:\n\n Learn prompting\n Prompting guide\n OpenAI cookbook"
  },
  {
    "objectID": "slides/presentation-1.html#chatgpt-edu",
    "href": "slides/presentation-1.html#chatgpt-edu",
    "title": "Presentation 1",
    "section": "ChatGPT Edu",
    "text": "ChatGPT Edu\n\n\n \n\n\n\n\nAccess to GPT-4o, excelling in text interpretation, coding, and mathematics\nData analytics, web browsing, and document summarization\nBuild GPTs, custom versions of ChatGPT, and share them within university workspaces\nSignificantly higher message limits than the free version of ChatGPT\nImproved language capabilities across quality and speed, with over 50 languages supported\nRobust security, data privacy, and administrative controls\nConversations and data are not used to train OpenAI models"
  },
  {
    "objectID": "slides/presentation-1.html#gpts",
    "href": "slides/presentation-1.html#gpts",
    "title": "Presentation 1",
    "section": "GPTs",
    "text": "GPTs"
  },
  {
    "objectID": "slides/presentation-1.html#hands-on-practice-gpts",
    "href": "slides/presentation-1.html#hands-on-practice-gpts",
    "title": "Presentation 1",
    "section": "Hands-on practice: GPTs",
    "text": "Hands-on practice: GPTs\n\nTry out custom GPTs from various categories in the GPT store.\nDiscuss with your neighbour\n\nDid you discover any useful GPTs?\nWhat are the benefits and limitations of using GPTs in the classroom?"
  },
  {
    "objectID": "slides/presentation-1.html#extended-cognition",
    "href": "slides/presentation-1.html#extended-cognition",
    "title": "Presentation 1",
    "section": "Extended cognition",
    "text": "Extended cognition\n\n\n\nAccording to Clark and Chalmers (1998), cognitive processes may extend to external objects.\nKrakauer (2016) distinguishes between complementary and competitive cognitive artifacts.\n\nComplementary: numbers, abacus\nCompetetive: calculator, GPS\n\nWhat kind of artefact will AI turn out to be?"
  },
  {
    "objectID": "slides/presentation-1.html#deskilling-vs.-upskilling",
    "href": "slides/presentation-1.html#deskilling-vs.-upskilling",
    "title": "Presentation 1",
    "section": "Deskilling vs.¬†upskilling",
    "text": "Deskilling vs.¬†upskilling"
  },
  {
    "objectID": "slides/presentation-1.html#writing-tasks-in-the-ai-era",
    "href": "slides/presentation-1.html#writing-tasks-in-the-ai-era",
    "title": "Presentation 1",
    "section": "Writing tasks in the AI era",
    "text": "Writing tasks in the AI era\n\nWriting is a core skill: critical thinking, persuasion, argumentation, understanding.\nText creation is secondary in learning: focus is on underlying skills.\nLearning objectives: Benefits of writing tasks should be clearly and convincingly conveyed.\nStudents should be equipped for effective (controlled) use of AI."
  },
  {
    "objectID": "slides/presentation-1.html#ai-can-do-my-homework",
    "href": "slides/presentation-1.html#ai-can-do-my-homework",
    "title": "Presentation 1",
    "section": "AI can do my homework",
    "text": "AI can do my homework\n\nWe can think of this as cheating.\nMore useful: cheating means bypassing useful cognition and therefore missing out on learning.\nCheating an ethics problem.\nBypassing cognition is a learning problem.\nNot a new problem: books, encyclopedias, calculators, spell checkers, etc."
  },
  {
    "objectID": "slides/presentation-1.html#controlled-use-of-llms",
    "href": "slides/presentation-1.html#controlled-use-of-llms",
    "title": "Presentation 1",
    "section": "Controlled use of LLMs",
    "text": "Controlled use of LLMs\n\n\n\n\n\n\n\nTask Category\nSpecific Tasks\n\n\n\n\nEditing tasks\nCreate/improve different versions of sections.\n\n\nTransitions\nWrite and compare transitions.\n\n\nImprove drafts\nCritique and refine drafts.\n\n\nWriting styles\nRewrite sections for different audiences.\n\n\nControversial statements\nIdentify controversial points and strengthen arguments.\n\n\nResearch journal\nKeep a diary and use LLM for reflection."
  },
  {
    "objectID": "slides/presentation-1.html#sport-vs.-writing",
    "href": "slides/presentation-1.html#sport-vs.-writing",
    "title": "Presentation 1",
    "section": "Sport vs.¬†writing",
    "text": "Sport vs.¬†writing\n\n\n\nTechnological advancements in sports: a useful analogy for learning?\nDistinction between training and competition.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLZR Racer swim suit\nAI-base writing tools\n\n\n\n\nImprovement\nReduced Resistance, Increased Buoyancy\nImproved Grammar, Formulation, Content Creation\n\n\nFairness\nProvided an Unfair Advantage, Led to Record Performances\nConsidered Unfair in Academic Contexts\n\n\nImpact\nBanned to Maintain Competitive Integrity\nRaises Questions of Originality and Skill Development"
  },
  {
    "objectID": "slides/presentation-1.html#understanding-the-value-of-effort",
    "href": "slides/presentation-1.html#understanding-the-value-of-effort",
    "title": "Presentation 1",
    "section": "Understanding the value of effort",
    "text": "Understanding the value of effort\n\nCheating can be a symptom that learners do not understand or value the importance of their own work.\nJust like in sport: if we take shortcuts during training, we won‚Äôt get fit.\nUnderstanding the purpose is important to endure discomfort.\nLearners need to understand what they are supposed to learn, why it is valuable, and why effort and discomfort are necessary."
  },
  {
    "objectID": "slides/presentation-1.html#fraud-triangle",
    "href": "slides/presentation-1.html#fraud-triangle",
    "title": "Presentation 1",
    "section": "Fraud triangle",
    "text": "Fraud triangle"
  },
  {
    "objectID": "slides/presentation-1.html#learning-environments-that-promote-cheating",
    "href": "slides/presentation-1.html#learning-environments-that-promote-cheating",
    "title": "Presentation 1",
    "section": "Learning Environments that promote cheating",
    "text": "Learning Environments that promote cheating\n\n\n\n\n\n\n\nFactors\nDescriptions\n\n\n\n\nHigh pressure\nHigh stakes increase cheating. Fear of failure reinforces this.\n\n\nLack of intrinsic motivation\nEngagement and relevance are important. Lacking these makes cheating more attractive.\n\n\nPerceived injustice\nUnfair grading leads to cheating.\n\n\nLow fear of getting caught\nLow risk encourages cheating.\n\n\nPeer influence\nWidespread cheating among peers pressures students to join in.\n\n\nLow self-efficacy\nDoubts about one‚Äôs own abilities increase cheating as the seemingly only option."
  },
  {
    "objectID": "slides/presentation-1.html#strategies-to-reduce-cheating",
    "href": "slides/presentation-1.html#strategies-to-reduce-cheating",
    "title": "Presentation 1",
    "section": "Strategies to Reduce Cheating",
    "text": "Strategies to Reduce Cheating\n\n\n\n\n\n\n\nStrategies\nDescriptions\n\n\n\n\nFoster intrinsic motivation\nSpark genuine interest. Provide choices and practical applications.\n\n\nMastery learning\nClear learning objectives. Focus on mastery of content. Include constructive and corrective feedback in formative assessments.\n\n\nReduce pressure\nDiversify assessment methods. Use portfolios and low-stress tests to reduce anxiety.\n\n\nStrengthen self-efficacy\nProvide constructive feedback and promote peer learning (peer tutoring, peer review).\n\n\nCreate a culture of integrity\nOpen discussion about academic integrity. Set clear guidelines and promote community ethics."
  },
  {
    "objectID": "slides/presentation-1.html#academic-integrity-plagiarism",
    "href": "slides/presentation-1.html#academic-integrity-plagiarism",
    "title": "Presentation 1",
    "section": "Academic Integrity: Plagiarism",
    "text": "Academic Integrity: Plagiarism\n\n\n\n\n\n\n\nTypes of Plagiarism\nDescription\n\n\n\n\nUnattributed use\nUsing the work or ideas of others without proper attribution.\n\n\nMinor changes or translations\nUsing the work of others with minor changes or translations without attribution.\n\n\nSelf-plagiarism\nReusing substantial parts of one‚Äôs own work without proper citation.\n\n\nJoint works\nReusing jointly written publications without proper acknowledgment."
  },
  {
    "objectID": "slides/presentation-1.html#academic-integrity-misconduct-in-authorship",
    "href": "slides/presentation-1.html#academic-integrity-misconduct-in-authorship",
    "title": "Presentation 1",
    "section": "Academic Integrity: Misconduct in authorship",
    "text": "Academic Integrity: Misconduct in authorship\n\n\n\n\n\n\n\nTypes of Plagiarism\nDescription\n\n\n\n\nUnattributed use\nUsing the work or ideas of others without proper attribution.\n\n\nMinor changes or translations\nUsing the work of others with minor changes or translations without attribution.\n\n\nSelf-plagiarism\nReusing substantial parts of one‚Äôs own work without proper citation.\n\n\nJoint works\nReusing jointly written publications without proper acknowledgment."
  },
  {
    "objectID": "slides/presentation-1.html#how-to-cite-chatgpt",
    "href": "slides/presentation-1.html#how-to-cite-chatgpt",
    "title": "Presentation 1",
    "section": "How to cite ChatGPT",
    "text": "How to cite ChatGPT\nE.g. APA Style: Cite as software (not as personal communication)."
  },
  {
    "objectID": "slides/presentation-1.html#documentating-ai-use",
    "href": "slides/presentation-1.html#documentating-ai-use",
    "title": "Presentation 1",
    "section": "Documentating AI use",
    "text": "Documentating AI use\n\nSpecifying prompts works well for inexperienced users, but inadequately reflects complex processes.\nExperienced users work with dialogues and several tools, not monolithic prompts in ChatGPT.\nWorking with copilot (code): no traceable prompt input.\nInstead: Document the process, including the tools used and the steps taken.\n\nInclude used tools and steps in appendix, with optional graphical representation.\nServes both evaluation and self-reflection.\n\nIs documentation meaningful in the long term, once the use of AI-based tools has become commonplace?"
  },
  {
    "objectID": "slides/presentation-1.html#detecting-ai-use",
    "href": "slides/presentation-1.html#detecting-ai-use",
    "title": "Presentation 1",
    "section": "Detecting AI use",
    "text": "Detecting AI use\n\nCan be detected by the use of specific vocabulary and phrases: ‚Äúdelve‚Äù, ‚Äúvibrant‚Äù, ‚Äúembark‚Äù, ‚Äúit‚Äôs important to note‚Äù, ‚Äù based on the data provided‚Äù.\nDetection tools are not very useful, and can be easily circumvented.\nAccording to Fleckenstein et al. (2024)\n\nGenerative AI can write papers that are undetectable.\nTeachers overestimate their detection abilities."
  },
  {
    "objectID": "slides/prompt-engineering-intermediate.html#references",
    "href": "slides/prompt-engineering-intermediate.html#references",
    "title": "Presentation 2",
    "section": "",
    "text": "Berner Fachhochschule | Bern University of Applied Sciences\n\n\n\n\nBowen, Jos√© Antonio, and C. Edward Watson. 2024. Teaching with AI. Johns Hopkins University Press. https://doi.org/10.56021/9781421449227.\n\n\nLang, James M. 2013. ‚ÄúCheating Lessons.‚Äù Harvard University Press. 2013. https://www.hup.harvard.edu/books/9780674724631.\n\n\nSchulhoff, Sander, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, et al. 2024. ‚ÄúThe Prompt Report: A Systematic Survey of Prompting Techniques.‚Äù June 6, 2024. http://arxiv.org/abs/2406.06608."
  },
  {
    "objectID": "pages/workshop-structure.html",
    "href": "pages/workshop-structure.html",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "",
    "text": "This document outlines the structure and activities for ‚ÄúThe Power of Prompts‚Äù workshop, designed to help educators understand and utilize Large Language Models (LLMs) effectively."
  },
  {
    "objectID": "pages/workshop-structure.html#setup",
    "href": "pages/workshop-structure.html#setup",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Setup",
    "text": "Setup\n\n\n\n\n\n\nTip\n\n\n\n\nVerify technical requirements\nEnsure LLM platform access\nSet up document sharing"
  },
  {
    "objectID": "pages/workshop-structure.html#opening-activity-llm-prediction-challenge",
    "href": "pages/workshop-structure.html#opening-activity-llm-prediction-challenge",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Opening Activity: LLM Prediction Challenge",
    "text": "Opening Activity: LLM Prediction Challenge\n\n\n\n\n\n\nNote\n\n\n\nBefore the main slides, engage participants with:\n\nDisplay 3 surprising LLM outputs\nGroup discussion: What prompts created these?\nCollect hypotheses\nReveal actual prompts"
  },
  {
    "objectID": "pages/workshop-structure.html#understanding-large-language-models-llms",
    "href": "pages/workshop-structure.html#understanding-large-language-models-llms",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Understanding Large Language Models (LLMs)",
    "text": "Understanding Large Language Models (LLMs)"
  },
  {
    "objectID": "pages/workshop-structure.html#prompt-engineering-basics",
    "href": "pages/workshop-structure.html#prompt-engineering-basics",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Prompt Engineering Basics",
    "text": "Prompt Engineering Basics"
  },
  {
    "objectID": "pages/workshop-structure.html#progressive-revelation-activity",
    "href": "pages/workshop-structure.html#progressive-revelation-activity",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Progressive Revelation Activity",
    "text": "Progressive Revelation Activity\n\n\n\n\n\n\nImportant\n\n\n\nStep-by-step prompt building (15 min):\n\nStart with a basic prompt\nAdd role specification\nAdd context details\nAdd output formatting\nCompare results at each stage"
  },
  {
    "objectID": "pages/workshop-structure.html#prompt-detective-exercise",
    "href": "pages/workshop-structure.html#prompt-detective-exercise",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Prompt Detective Exercise",
    "text": "Prompt Detective Exercise\n\n\n\n\n\n\nImportant\n\n\n\nReverse engineering activity (15 min):\n\nGroups receive successful outputs\nTry to reconstruct prompts\nTest their versions\nCompare with original\nDiscuss differences"
  },
  {
    "objectID": "pages/workshop-structure.html#intermediate-techniques",
    "href": "pages/workshop-structure.html#intermediate-techniques",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Intermediate Techniques",
    "text": "Intermediate Techniques"
  },
  {
    "objectID": "pages/workshop-structure.html#error-first-learning-activity",
    "href": "pages/workshop-structure.html#error-first-learning-activity",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Error-First Learning Activity",
    "text": "Error-First Learning Activity\n\n\n\n\n\n\nImportant\n\n\n\nLearning from mistakes (20 min):\n\nPresent common error scenarios\n\nVague instructions\nMissing context\nPoor formatting\n\nGroups attempt fixes\nTest solutions\nDocument improvements"
  },
  {
    "objectID": "pages/workshop-structure.html#prompt-surgery-workshop",
    "href": "pages/workshop-structure.html#prompt-surgery-workshop",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Prompt Surgery Workshop",
    "text": "Prompt Surgery Workshop\n\n\n\n\n\n\nImportant\n\n\n\nFixing broken prompts (25 min):\n\nExamine problematic prompts\nDiagnose issues\nApply fixes\nTest and refine\nShare successful strategies"
  },
  {
    "objectID": "pages/workshop-structure.html#anki-flashcard-project",
    "href": "pages/workshop-structure.html#anki-flashcard-project",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Anki Flashcard Project",
    "text": "Anki Flashcard Project\n\n\n\n\n\n\nPhase 1: Individual Design (15 min)\n\n\n\n\nSelect teaching material\nApply all techniques learned\nCreate initial flashcard prompt\nTest and document results\n\n\nPhase 2: Group Refinement (15 min)\n\nForm groups of 3-4\nShare and test prompts\nPeer review using checklist\nRefine based on feedback\n\n\n\nPhase 3: Technical Implementation (15 min)\n\nFormat for Anki import\nTest import process\nTroubleshoot issues\nDocument best practices"
  },
  {
    "objectID": "pages/workshop-structure.html#teaching-scenario-planning",
    "href": "pages/workshop-structure.html#teaching-scenario-planning",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Teaching Scenario Planning",
    "text": "Teaching Scenario Planning\n\n\n\n\n\n\nImportant\n\n\n\nIndividual planning (10 min):\n\nChoose specific teaching task\nDesign implementation plan\nNote potential challenges\nIdentify success metrics"
  },
  {
    "objectID": "pages/workshop-structure.html#final-wrap-up",
    "href": "pages/workshop-structure.html#final-wrap-up",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Final Wrap-Up",
    "text": "Final Wrap-Up\n\n\n\n\n\n\nTip\n\n\n\nGroup reflection (5 min):\n\nKey takeaways\nSuccess strategies\nNext steps\nResource sharing"
  },
  {
    "objectID": "pages/workshop-structure.html#room-layout",
    "href": "pages/workshop-structure.html#room-layout",
    "title": "The Power of Prompts: Workshop Structure",
    "section": "Room Layout",
    "text": "Room Layout\n\nU-shape or groups\nScreen visibility\nMovement space\nPower access"
  },
  {
    "objectID": "pages/index.html#opening-activity-llm-prediction-challenge",
    "href": "pages/index.html#opening-activity-llm-prediction-challenge",
    "title": "Using AI in Education: Intermediate",
    "section": "Opening Activity: LLM Prediction Challenge",
    "text": "Opening Activity: LLM Prediction Challenge\n\n\n\n\n\n\nGroup Discussion\n\n\n\nLLM Prediction Challenge (10 min)\n\nDisplay 3 surprising LLM outputs\nGroup discussion: What prompts created these?\nCollect hypotheses\nReveal actual prompts"
  },
  {
    "objectID": "pages/index.html#understanding-large-language-models-llms",
    "href": "pages/index.html#understanding-large-language-models-llms",
    "title": "Using AI in Education: Intermediate",
    "section": "Understanding Large Language Models (LLMs)",
    "text": "Understanding Large Language Models (LLMs)\n    View slides in full screen"
  },
  {
    "objectID": "pages/index.html#prompt-engineering-basics",
    "href": "pages/index.html#prompt-engineering-basics",
    "title": "Using AI in Education: Intermediate",
    "section": "Prompt Engineering Basics",
    "text": "Prompt Engineering Basics\n    View slides in full screen"
  },
  {
    "objectID": "pages/index.html#progressive-revelation-activity",
    "href": "pages/index.html#progressive-revelation-activity",
    "title": "Using AI in Education: Intermediate",
    "section": "Progressive Revelation Activity",
    "text": "Progressive Revelation Activity\n\n\n\n\n\n\nHands-On Practice\n\n\n\nProgressive Revelation Activity (15 min)\nStep-by-step prompt building:\n\nStart with basic prompt\nAdd role specification\nAdd context details\nAdd output formatting\nCompare results at each stage"
  },
  {
    "objectID": "pages/index.html#prompt-detective-exercise",
    "href": "pages/index.html#prompt-detective-exercise",
    "title": "Using AI in Education: Intermediate",
    "section": "Prompt Detective Exercise",
    "text": "Prompt Detective Exercise\n\n\n\n\n\n\nGroup Work\n\n\n\nPrompt Detective Exercise (15 min)\n\nGroups receive successful outputs\nTry to reconstruct prompts\nTest their versions\nCompare with original\nDiscuss differences"
  },
  {
    "objectID": "pages/index.html#intermediate-techniques",
    "href": "pages/index.html#intermediate-techniques",
    "title": "Using AI in Education: Intermediate",
    "section": "Intermediate Techniques",
    "text": "Intermediate Techniques\n    View slides in full screen"
  },
  {
    "objectID": "pages/index.html#error-first-learning-activity",
    "href": "pages/index.html#error-first-learning-activity",
    "title": "Using AI in Education: Intermediate",
    "section": "Error-First Learning Activity",
    "text": "Error-First Learning Activity\n\n\n\n\n\n\nLearning from Mistakes\n\n\n\nError-First Learning Activity (20 min)\n\nPresent common error scenarios:\n\nVague instructions\nMissing context\nPoor formatting\n\nGroups attempt fixes\nTest solutions\nDocument improvements"
  },
  {
    "objectID": "pages/index.html#prompt-surgery-workshop",
    "href": "pages/index.html#prompt-surgery-workshop",
    "title": "Using AI in Education: Intermediate",
    "section": "Prompt Surgery Workshop",
    "text": "Prompt Surgery Workshop\n\n\n\n\n\n\nCollaborative Analysis\n\n\n\nPrompt Surgery Workshop (25 min)\n\nExamine problematic prompts\nDiagnose issues\nApply fixes\nTest and refine\nShare successful strategies"
  },
  {
    "objectID": "pages/index.html#anki-flashcard-project",
    "href": "pages/index.html#anki-flashcard-project",
    "title": "Using AI in Education: Intermediate",
    "section": "Anki Flashcard Project",
    "text": "Anki Flashcard Project\n\n\n\n\n\n\nProject Work\n\n\n\nAnki Flashcard Project - Phase 1 (15 min)\n\nSelect teaching material\nApply learned techniques\nCreate initial flashcard prompt\nTest and document results\n\n\n\n\n\n\n\n\n\nPeer Review\n\n\n\nGroup Refinement - Phase 2 (15 min)\n\nForm groups of 3-4\nShare and test prompts\nPeer review using checklist\nRefine based on feedback\n\n\n\n\n\n\n\n\n\nTechnical Implementation\n\n\n\n\n\nExport Phase - Phase 3 (15 min)\n\nFormat for Anki import\nTest import process\nTroubleshoot issues\nDocument best practices"
  },
  {
    "objectID": "pages/index.html#teaching-scenario-planning",
    "href": "pages/index.html#teaching-scenario-planning",
    "title": "Using AI in Education: Intermediate",
    "section": "Teaching Scenario Planning",
    "text": "Teaching Scenario Planning\n\n\n\n\n\n\nPlanning Ahead\n\n\n\nTeaching Scenario Planning (10 min)\n\nChoose specific teaching task\nDesign implementation plan\nNote potential challenges\nIdentify success metrics"
  },
  {
    "objectID": "pages/index.html#final-wrap-up",
    "href": "pages/index.html#final-wrap-up",
    "title": "Using AI in Education: Intermediate",
    "section": "Final Wrap-Up",
    "text": "Final Wrap-Up\n\n\n\n\n\n\nWrap-Up\n\n\n\nFinal Review (5 min)\n\nKey takeaways\nSuccess strategies\nNext steps\nResource sharing\n\n\n\n\nDesigning effective prompts to instruct LLMs to generate a desired output is referred to as prompt engineering. This activity will guide you through the process of creating prompts for LLMs.\n\n\n\n\n\n\nTip\n\n\n\n\nIn general, it is advisable to improve your prompting strategy by iterating.\nStart with a simple prompt, see what the model generates, and then refine. Repeat this process until you are satisfied with the results.\nOnce you have a good prompt, you can use it in a new chat session - in this way, the ‚Äúfailed‚Äù attempts will not be part of the context."
  },
  {
    "objectID": "pages/index.html#session-overview",
    "href": "pages/index.html#session-overview",
    "title": "Using AI in Education: Intermediate",
    "section": "",
    "text": "Opening Block (15 min)\nFoundations Block (45 min)\nBreak (15 min)\nIntermediate Block (45 min)\nImplementation Block (45 min)\nClosing (15 min)"
  },
  {
    "objectID": "pages/index.html#progressive-revelation-activity-15-min",
    "href": "pages/index.html#progressive-revelation-activity-15-min",
    "title": "Using AI in Education: Intermediate",
    "section": "Progressive Revelation Activity (15 min)",
    "text": "Progressive Revelation Activity (15 min)\nStep-by-step prompt building:\n\nStart with basic prompt\nAdd role specification\nAdd context details\nAdd output formatting\nCompare results at each stage"
  },
  {
    "objectID": "pages/index.html#prompt-detective-exercise-15-min",
    "href": "pages/index.html#prompt-detective-exercise-15-min",
    "title": "Using AI in Education: Intermediate",
    "section": "Prompt Detective Exercise (15 min)",
    "text": "Prompt Detective Exercise (15 min)\n\nGroups receive successful outputs\nTry to reconstruct prompts\nTest their versions\nCompare with original\nDiscuss differences"
  },
  {
    "objectID": "pages/index.html#break-15-min-1",
    "href": "pages/index.html#break-15-min-1",
    "title": "Using AI in Education: Intermediate",
    "section": "Break (15 min)",
    "text": "Break (15 min)\n\nMovement encouraged\nOptional experiments displayed\nTechnical support available"
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Virtuelle Akademie",
    "section": "",
    "text": "KI Orientierungshilfe der BFH\n\nKI-basierte Schreibtools in der Lehre ‚Äì ChatGPT im Fokus\nKI-basierte Schreibtools in der Lehre ‚Äì Knowledge Base\n\n\n\nBildung 6.0\nDas Projekt Bildung 6.0 der Berner Fachhochschule stellt relevante und verl√§ssliche Informationen und Empfehlungen zum richtigen Umgang mit KI-basierten Werkzeugen (KBW) f√ºr Studierende und Lehrende auf einer Online-Plattform bereit.\n\nProjekt Bildung 6.0\n\n\n\nWissenschaftliches Arbeiten\n\nChatGPT zitieren\nRechtliche Fragen\nDidaktische Und Rechtliche Perspektiven Auf Ki-Gest√ºtztes Schreiben In Der Hochschulbildung (Salden 2023)\n\n\n\nPrompt Engineering\nExplore these prompt guides\n    View webpage in full screen\n       \n      \n    \n\n\n\n\n\n\nLearn prompting: An comprehensive (and free) guide aimed at non-technical users.\n\n\n\n\n\n Learn prompting\n\n\n\n\n\n\n\n\n\n\nPrompting guide: A more technical guide to prompting\n\n\n\n\n\n Prompting guide: DAIR.AI (Democratizing Artificial Intelligence Research, Education, and Technologies). The guide is licensed under an MIT license.\n\n\n\n\n\n\n\n\n Back to topReferences\n\nSalden, Peter. 2023. ‚ÄúDidaktische und rechtliche Perspektiven auf KI-gest√ºtztes Schreiben in der Hochschulbildung.‚Äù Ruhr-Universit√§t Bochum. https://doi.org/10.13154/294-9734."
  },
  {
    "objectID": "slides/understanding-llms.html#what-is-a-large-language-model",
    "href": "slides/understanding-llms.html#what-is-a-large-language-model",
    "title": "Understanding Large Language Models",
    "section": "What is a Large Language Model?",
    "text": "What is a Large Language Model?\n\nNeural network trained on vast amounts of text data.\nOutput a distribution over all possible tokens in a sequence conditional on input.\nLLMs undergo three key stages of training:\n\n\n\n\n\n\nflowchart LR\n    A[\"1. Pre-training&lt;br/&gt;Trained on large text&lt;br/&gt;corpora&lt;br/&gt;Task: next token prediction\"] --&gt; \n    B[\"2. Instruction fine-tuning&lt;br/&gt;Trained on in-/output pairs&lt;br/&gt;Task: next token prediction\"] --&gt;\n    C[\"3. Reinforcement learning&lt;br/&gt;From human feedback&lt;br/&gt;Trained on ratings&lt;br/&gt;Aligns model outputs with&lt;br/&gt;human preferences\"]\n\n    style A fill:#e5e9eb\n    style B fill:#e6eeff\n    style C fill:#f3e6ff"
  },
  {
    "objectID": "slides/understanding-llms.html#questions-discussion",
    "href": "slides/understanding-llms.html#questions-discussion",
    "title": "Understanding Large Language Models",
    "section": "Questions / Discussion ",
    "text": "Questions / Discussion"
  },
  {
    "objectID": "slides/understanding-llms.html#how-do-llms-generate-text",
    "href": "slides/understanding-llms.html#how-do-llms-generate-text",
    "title": "Understanding Large Language Models",
    "section": "How do LLMs generate text?",
    "text": "How do LLMs generate text?\n\\[\n\\newcommand{\\purple}[1]{\\color{purple}{#1}}\n\\newcommand{\\red}[1]{\\color{red}{#1}}\n\\newcommand{\\blue}[1]{\\color{blue}{#1}}\n\\]\n\\[\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\]\n\n\\(\\purple{\\text{Next token}}\\)\n\\(\\blue{\\text{The input sequence}:\\ x_1, \\ldots, x_i}\\)\n\\(\\red{\\text{The learned model}}\\)"
  },
  {
    "objectID": "slides/understanding-llms.html#how-do-llms-generate-text-1",
    "href": "slides/understanding-llms.html#how-do-llms-generate-text-1",
    "title": "Understanding Large Language Models",
    "section": "How do LLMs generate text?",
    "text": "How do LLMs generate text?\n\\[\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\]"
  },
  {
    "objectID": "slides/understanding-llms.html#the-space-of-all-possible-outputs",
    "href": "slides/understanding-llms.html#the-space-of-all-possible-outputs",
    "title": "Understanding Large Language Models",
    "section": "The Space Of All Possible Outputs",
    "text": "The Space Of All Possible Outputs\n\n\nFigure from Shanahan, McDonell, and Reynolds (2023)\n\n\nAn LLM has the capacity to generate any number of possible documents based on its training. However, the context we provide narrows down the potential outputs by guiding the model along specific trajectories.\n- Without context: All paths are equally possible.\n- With context: The paths are constrained, reducing the potential outcomes.\n\nFigure Description: A branching tree diagram. At the base (without context), the tree splits into many branches, each representing a possible document. As tokens (context) are added, paths narrow, showing fewer branches remaining as viable output possibilities."
  },
  {
    "objectID": "slides/understanding-llms.html#what-do-llms-learn",
    "href": "slides/understanding-llms.html#what-do-llms-learn",
    "title": "Understanding Large Language Models",
    "section": "What Do LLMs Learn?",
    "text": "What Do LLMs Learn?\n\\[\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\]\n\nKnowledge is statistical and not grounded in direct experience or sensory input\n\nSyntax and grammar\nSemantics, pragmatics\nAssociations between concepts, facts, and common knowledge\n\nThey can simulate reasoning and planning, but can they actually reason and plan (Kambhampati 2024)?"
  },
  {
    "objectID": "slides/understanding-llms.html#example",
    "href": "slides/understanding-llms.html#example",
    "title": "Understanding Large Language Models",
    "section": "Example",
    "text": "Example\n\n\n\nExample Prompt\n\n\nA farmer needs to transport a wolf, a goat, and a cabbage across a river using a small boat. The boat can carry only the farmer and one other item (either the wolf, the goat, or the cabbage) at a time. However, if left together without the farmer‚Äôs supervision: The wolf will eat the cabbage. The goat will eat the wolf. The challenge is to figure out how the farmer can get all three items across the river safely. What is the sequence of moves that the farmer needs to make?\n\n\n\n\n\n\nOutput Example\n\n\nThis is a classic puzzle with a twist! Here‚Äôs the step-by-step solution for safely transporting the wolf, goat, and cabbage across the river:\n1.  First trip: The farmer takes the goat across the river and leaves it on the other side.\n2.  Second trip: The farmer returns alone to the original side.\n3.  Third trip: The farmer takes the wolf across the river.\n4.  Fourth trip: The farmer leaves the wolf on the other side but takes the goat back to the original side.\n5.  Fifth trip: The farmer takes the cabbage across the river and leaves it with the wolf.\n6.  Sixth trip: The farmer returns alone to the original side.\n7.  Seventh trip: The farmer takes the goat across the river.\nNow, all three (wolf, goat, and cabbage) are safely across the river, and the puzzle is solved!\nKey point: By taking the goat back on the fourth trip, the farmer ensures that neither the wolf nor the goat are left together, and neither are the goat and the cabbage left together."
  },
  {
    "objectID": "slides/understanding-llms.html#can-llms-reason",
    "href": "slides/understanding-llms.html#can-llms-reason",
    "title": "Understanding Large Language Models",
    "section": "Can LLMs Reason?",
    "text": "Can LLMs Reason?\n\n\n\nExample Prompt\n\n\nA farmer needs to transport a wolf, a goat, and a cabbage across a river using a small boat. The boat can carry only the farmer and one other item (either the wolf, the goat, or the cabbage) at a time. However, if left together without the farmer‚Äôs supervision: The wolf will eat the cabbage. The goat will eat the wolf. The challenge is to figure out how the farmer can get all three items across the river safely. What is the sequence of moves that the farmer needs to make?\n\n\n\n\n\n\nOutput Example\n\n\nThis is a classic puzzle with a twist! Here‚Äôs the step-by-step solution for safely transporting the wolf, goat, and cabbage across the river:\n1.  First trip: The farmer takes the goat across the river and leaves it on the other side.\n2.  Second trip: The farmer returns alone to the original side.\n3.  Third trip: The farmer takes the wolf across the river.\n4.  Fourth trip: The farmer leaves the wolf on the other side but takes the goat back to the original side.\n5.  Fifth trip: The farmer takes the cabbage across the river and leaves it with the wolf.\n6.  Sixth trip: The farmer returns alone to the original side.\n7.  Seventh trip: The farmer takes the goat across the river.\nNow, all three (wolf, goat, and cabbage) are safely across the river, and the puzzle is solved!\nKey point: By taking the goat back on the fourth trip, the farmer ensures that neither the wolf nor the goat are left together, and neither are the goat and the cabbage left together.\n\n\n\n A wolf, goat, and cabbage problem with a twist"
  },
  {
    "objectID": "slides/understanding-llms.html#manhole-covers-why-are-they-round",
    "href": "slides/understanding-llms.html#manhole-covers-why-are-they-round",
    "title": "Understanding Large Language Models",
    "section": "Manhole Covers: Why Are They Round?",
    "text": "Manhole Covers: Why Are They Round?"
  },
  {
    "objectID": "slides/understanding-llms.html#probabilistic-reasoning",
    "href": "slides/understanding-llms.html#probabilistic-reasoning",
    "title": "Understanding Large Language Models",
    "section": "Probabilistic Reasoning",
    "text": "Probabilistic Reasoning\n\n\n\nA tool capable of reasoning about possible causes of observable symptoms must perform probabilistic reasoning.\nLLMs cannot do this,"
  },
  {
    "objectID": "slides/understanding-llms.html#manhole-covers-why-are-they-round-1",
    "href": "slides/understanding-llms.html#manhole-covers-why-are-they-round-1",
    "title": "Understanding Large Language Models",
    "section": "Manhole Covers: Why Are They Round?",
    "text": "Manhole Covers: Why Are They Round?\n\n\n\n\n\nThe human body that had to fit through the hole is round.\nA circle gives a maximum opening for a minimum surface area.\nHeavy round covers can be rolled rather than lifted into position.\nThey were easier and cheaper to manufacture than square pit covers of the same materials.\nA circle has a constant diameter and won‚Äôt fall into the pit if rotatedthe wrong way ‚Äì a tricky problem to remedy if the access cover weighs well over 100kg as concrete and iron covers did.\n\n Why are manhole covers round?"
  },
  {
    "objectID": "slides/understanding-llms.html#prompting-guiding-the-model-through-its-space-of-possible-outputs",
    "href": "slides/understanding-llms.html#prompting-guiding-the-model-through-its-space-of-possible-outputs",
    "title": "Understanding Large Language Models",
    "section": "Prompting: Guiding the Model Through its Space of Possible Outputs",
    "text": "Prompting: Guiding the Model Through its Space of Possible Outputs\n\\(\\purple{P(x_{i+1}} \\mid \\blue{\\text{Context}}, \\red{\\text{Model}})\\)\n\nPrompting guides the LLM along specific paths in its space of possible documents.\nEvery token in a prompt reduces the number of potential outcomes, helping the model generate relevant responses.\n\nWithout a prompt, all outputs are possible.\nAs tokens are added, the range of possible outputs shrinks, making the model‚Äôs behavior more predictable."
  },
  {
    "objectID": "slides/understanding-llms.html#how-prompting-reduces-uncertainty",
    "href": "slides/understanding-llms.html#how-prompting-reduces-uncertainty",
    "title": "Understanding Large Language Models",
    "section": "How Prompting Reduces Uncertainty",
    "text": "How Prompting Reduces Uncertainty\n\nEach token conditions the model‚Äôs next prediction\nWith more context, the uncertainty (entropy) decreases, guiding the model towards a more specific output."
  },
  {
    "objectID": "slides/understanding-llms.html#the-power-of-prompting-why-it-matters",
    "href": "slides/understanding-llms.html#the-power-of-prompting-why-it-matters",
    "title": "Understanding Large Language Models",
    "section": "The Power of Prompting: Why It Matters",
    "text": "The Power of Prompting: Why It Matters\n\nControls the behaviour of LLMs, steering them toward relevant outputs.\nWithout effective prompting, the full potential of an LLM remains untapped, as it may generate irrelevant or misleading outputs.\n\nPrompting allows us to: - Navigate the vast space of possible outputs. - Achieve more controlled and useful results.\nContexts are combinatorial: we do not know how a model will behave conditioned on all possible contexts. The output is highly contingent on the prompt."
  },
  {
    "objectID": "slides/understanding-llms.html#prompt-engineering",
    "href": "slides/understanding-llms.html#prompt-engineering",
    "title": "Understanding Large Language Models",
    "section": " Prompt Engineering",
    "text": "Prompt Engineering\nWe treat LLMs as black boxes and use engineering approaches to guide their behaviour."
  }
]